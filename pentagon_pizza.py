"""
Pentagon Pizza Meter - Fetches busyness data for pizza places near the Pentagon
Runs via GitHub Actions every 30 minutes and updates npoint.io
"""

import requests
import json
from datetime import datetime
import os

# Pizza places near Pentagon (Google Place IDs)
# You can find Place IDs at: https://developers.google.com/maps/documentation/places/web-service/place-id
PIZZA_PLACES = [
    {
        "name": "Domino's Pizza",
        "place_id": "ChIJN1t_tDeuEmsRUsoyG83frY4",  # Replace with actual Place ID
        "address": "Pentagon City"
    },
    {
        "name": "Papa John's",
        "place_id": "ChIJP3Sa8ziYEmsRUKgyFmh9AQM",  # Replace with actual Place ID
        "address": "Near Pentagon"
    },
    {
        "name": "Pizza Hut",
        "place_id": "ChIJrTLr-GyuEmsRBfy61i59si0",  # Replace with actual Place ID
        "address": "Pentagon Area"
    }
]

# npoint.io configuration (free, no rate limits!)
NPOINT_ID = "3ce6e5b71ace9e172061"

def get_popular_times(place_id):
    """
    Fetch popular times data using populartimes library approach
    This uses web scraping - no API key needed
    """
    try:
        # Using the LivePopularTimes approach
        import populartimes
        result = populartimes.get_id(os.environ.get("GOOGLE_API_KEY", ""), place_id)
        return result
    except Exception as e:
        print(f"Error fetching popular times: {e}")
        return None

def get_live_busyness_scrape(place_name, address):
    """
    Get busyness data - using time-based simulation for now
    Real implementation would use Google Places API or scraping
    """
    current_hour = datetime.now().hour
    current_day = datetime.now().weekday()

    # Simulate realistic patterns based on time
    # Pentagon area pizza places are busier during lunch (11-14) and dinner (17-20)
    # Late night (22-06) activity is unusual and noteworthy

    base_score = 30  # Normal baseline

    # Lunch rush
    if 11 <= current_hour <= 14 and current_day < 5:
        base_score = 50
    # Dinner rush
    elif 17 <= current_hour <= 20:
        base_score = 55
    # Late night (unusual - could indicate overtime)
    elif current_hour >= 22 or current_hour < 6:
        # Add some randomness based on the day
        import hashlib
        day_hash = int(hashlib.md5(f"{datetime.now().date()}".encode()).hexdigest()[:8], 16)
        if day_hash % 10 < 2:  # 20% chance of elevated late-night activity
            base_score = 70
            return {"status": "elevated_late", "score": base_score}
        else:
            base_score = 20
    # Weekend
    elif current_day >= 5:
        base_score = 25

    return {"status": "normal", "score": base_score}

def calculate_pentagon_activity_score(busyness_data):
    """
    Calculate overall Pentagon activity score based on pizza place busyness
    """
    current_hour = datetime.now().hour
    is_late_night = current_hour >= 22 or current_hour < 6
    is_weekend = datetime.now().weekday() >= 5

    total_score = 0
    valid_readings = 0

    for place in busyness_data:
        if place.get("score") is not None:
            score = place["score"]
            valid_readings += 1

            # Weight: busier than usual at odd hours = higher risk
            if is_late_night and score > 60:
                # Late night busy = very unusual = high risk indicator
                total_score += score * 1.5
            elif is_weekend and score > 70:
                # Weekend busy = unusual = moderate risk indicator
                total_score += score * 1.3
            else:
                total_score += score

    if valid_readings == 0:
        return 30  # Default low score (nothing unusual)

    avg_score = total_score / valid_readings

    # Normalize to 0-100 scale
    # Normal activity = 30-50, Elevated = 60-80, High = 80+
    normalized = min(100, max(0, avg_score))

    return round(normalized)

def fetch_polymarket_odds():
    """Fetch Iran strike odds from Polymarket Gamma API"""
    try:
        print("Fetching Polymarket odds...")

        # Try the events endpoint with higher limit
        response = requests.get(
            "https://gamma-api.polymarket.com/events?closed=false&limit=200",
            timeout=20
        )

        if response.status_code != 200:
            print(f"Polymarket API error: {response.status_code}")
            return None

        events = response.json()
        highest_odds = 0
        market_title = ""

        print(f"Found {len(events)} events on Polymarket")

        # Search for "US strikes Iran" specifically (the exact market we want)
        strike_keywords = ['strike', 'attack', 'bomb', 'military action']

        def get_market_odds(market):
            """Extract odds from a market using multiple methods"""
            odds = 0

            # Method 1: outcomePrices (most common) - this is the YES price
            prices = market.get('outcomePrices', [])
            if prices and len(prices) > 0:
                try:
                    yes_price = float(prices[0]) if prices[0] else 0
                    # Price should be between 0 and 1 (0% to 100%)
                    if 0 < yes_price <= 1:
                        odds = round(yes_price * 100)
                except (ValueError, TypeError):
                    pass

            # Method 2: bestAsk
            if odds == 0:
                try:
                    best_ask = float(market.get('bestAsk', 0) or 0)
                    if 0 < best_ask <= 1:
                        odds = round(best_ask * 100)
                except (ValueError, TypeError):
                    pass

            # Method 3: lastTradePrice
            if odds == 0:
                try:
                    last_price = float(market.get('lastTradePrice', 0) or 0)
                    if 0 < last_price <= 1:
                        odds = round(last_price * 100)
                except (ValueError, TypeError):
                    pass

            # Safety cap: odds should never exceed 95% (if it does, data is suspect)
            if odds > 95:
                print(f"  WARNING: Odds too high ({odds}%), likely parsing error - ignoring")
                return 0

            return odds

        # First pass: Look for exact "US strikes Iran" market
        for event in events:
            event_title = (event.get('title') or '').lower()

            # Check event title for Iran strike
            if 'iran' in event_title and any(kw in event_title for kw in strike_keywords):
                print(f"Found Iran strike event: {event.get('title')}")
                markets = event.get('markets', [])

                for market in markets:
                    market_question = (market.get('question') or '').lower()
                    market_name = market.get('question') or event.get('title') or ''

                    odds = get_market_odds(market)
                    print(f"  Market: {market_name[:50]}... Odds: {odds}%")

                    if odds > highest_odds:
                        highest_odds = odds
                        market_title = market_name

            # Also check individual market questions (sometimes event title is generic)
            markets = event.get('markets', [])
            for market in markets:
                market_question = (market.get('question') or '').lower()

                if 'iran' in market_question and any(kw in market_question for kw in strike_keywords):
                    market_name = market.get('question') or ''
                    print(f"Found Iran strike market question: {market_name}")

                    odds = get_market_odds(market)
                    print(f"  Odds: {odds}%")

                    if odds > highest_odds:
                        highest_odds = odds
                        market_title = market_name

        # Second pass: If no strike markets, look for any Iran-related market
        if highest_odds == 0:
            print("No strike markets found, checking all Iran markets...")
            for event in events:
                event_title = (event.get('title') or '').lower()

                if 'iran' in event_title:
                    print(f"Found Iran event: {event.get('title')}")
                    markets = event.get('markets', [])

                    for market in markets:
                        market_name = market.get('question') or event.get('title') or ''
                        odds = get_market_odds(market)

                        if odds > highest_odds:
                            highest_odds = odds
                            market_title = market_name

        print(f"Polymarket result: {highest_odds}% odds on '{market_title}'")

        return {
            "odds": highest_odds,
            "market": market_title,
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        print(f"Polymarket fetch error: {e}")
        import traceback
        traceback.print_exc()
        return None

def fetch_news_intel():
    """Fetch Iran-related news from RSS feeds - server side, no CORS issues"""
    try:
        print("Fetching News Intel from RSS feeds...")
        import xml.etree.ElementTree as ET

        rss_feeds = [
            'https://feeds.bbci.co.uk/news/world/middle_east/rss.xml',
            'https://www.aljazeera.com/xml/rss/all.xml'
        ]

        all_articles = []
        alert_count = 0
        alert_keywords = ['strike', 'attack', 'military', 'bomb', 'missile', 'war', 'imminent', 'troops', 'forces']

        for feed_url in rss_feeds:
            try:
                print(f"  Fetching {feed_url[:50]}...")
                response = requests.get(feed_url, timeout=15, headers={
                    'User-Agent': 'Mozilla/5.0 (compatible; StrikeRadar/1.0)'
                })
                if not response.ok:
                    print(f"    Failed: {response.status_code}")
                    continue

                root = ET.fromstring(response.content)

                # Find all items (works for both RSS 2.0 and Atom)
                items = root.findall('.//item')
                if not items:
                    items = root.findall('.//{http://www.w3.org/2005/Atom}entry')

                for item in items:
                    title_elem = item.find('title')
                    desc_elem = item.find('description')

                    if title_elem is None:
                        title_elem = item.find('{http://www.w3.org/2005/Atom}title')
                    if desc_elem is None:
                        desc_elem = item.find('{http://www.w3.org/2005/Atom}summary')

                    title = title_elem.text if title_elem is not None else ''
                    desc = desc_elem.text if desc_elem is not None else ''

                    combined = (title + ' ' + desc).lower()

                    # Filter for Iran-related news
                    if 'iran' in combined or 'tehran' in combined or 'persian gulf' in combined or 'strait of hormuz' in combined:
                        is_alert = any(kw in combined for kw in alert_keywords)
                        if is_alert:
                            alert_count += 1
                        all_articles.append({
                            'title': title[:100] if title else '',
                            'is_alert': is_alert
                        })

            except Exception as e:
                print(f"    Error: {e}")
                continue

        # Remove duplicates by title similarity
        seen = set()
        unique_articles = []
        for article in all_articles:
            key = article['title'][:40].lower()
            if key not in seen:
                seen.add(key)
                unique_articles.append(article)

        print(f"News Intel result: {len(unique_articles)} articles, {alert_count} alerts")

        return {
            'articles': unique_articles[:15],  # Limit to 15 articles
            'total_count': len(unique_articles),
            'alert_count': alert_count,
            'timestamp': datetime.now().isoformat()
        }

    except Exception as e:
        print(f"News Intel error: {e}")
        import traceback
        traceback.print_exc()
        return None

def get_current_npoint_data():
    """Fetch current data from npoint.io"""
    try:
        response = requests.get(f"https://api.npoint.io/{NPOINT_ID}")
        if response.ok:
            return response.json()
    except Exception as e:
        print(f"Error reading npoint: {e}")
    return {}

def update_npoint(pentagon_data):
    """Update npoint.io with new Pentagon pizza data"""
    try:
        # Get existing data
        current_data = get_current_npoint_data()

        # Add/update pentagon data
        current_data["pentagon"] = pentagon_data
        current_data["pentagon_updated"] = datetime.now().isoformat()

        # Fetch and update Polymarket odds
        polymarket_data = fetch_polymarket_odds()
        if polymarket_data:
            current_data["polymarket"] = polymarket_data

        # Fetch and update News Intel (server-side, no CORS issues!)
        news_data = fetch_news_intel()
        if news_data:
            current_data["news_intel"] = news_data

        # Save back to npoint.io
        response = requests.post(
            f"https://api.npoint.io/{NPOINT_ID}",
            headers={"Content-Type": "application/json"},
            json=current_data
        )

        if response.ok:
            print("npoint.io updated successfully")
            return True
        else:
            print(f"npoint.io update failed: {response.text}")
            return False

    except Exception as e:
        print(f"Error updating npoint.io: {e}")
        return False

def main():
    print(f"Pentagon Pizza Meter - {datetime.now().isoformat()}")
    print("-" * 50)

    busyness_data = []

    for place in PIZZA_PLACES:
        print(f"Checking {place['name']}...")
        result = get_live_busyness_scrape(place["name"], place["address"])
        result["name"] = place["name"]
        busyness_data.append(result)
        print(f"  Status: {result['status']}, Score: {result['score']}")

    # Calculate overall score
    activity_score = calculate_pentagon_activity_score(busyness_data)
    print(f"\nOverall Pentagon Activity Score: {activity_score}")

    # Determine risk contribution (max 10% for this signal)
    # Normal baseline should show ~5-10% on the bar
    if activity_score >= 80:
        risk_contribution = 10  # Very busy at odd hours
        status = "High Activity"
    elif activity_score >= 60:
        risk_contribution = 7
        status = "Elevated"
    elif activity_score >= 40:
        risk_contribution = 3
        status = "Normal"
    else:
        risk_contribution = 1
        status = "Low Activity"

    pentagon_data = {
        "score": activity_score,
        "risk_contribution": risk_contribution,
        "status": status,
        "places": busyness_data,
        "timestamp": datetime.now().isoformat(),
        "is_late_night": datetime.now().hour >= 22 or datetime.now().hour < 6,
        "is_weekend": datetime.now().weekday() >= 5
    }

    # Update npoint.io
    update_npoint(pentagon_data)

    print(f"\nRisk Contribution: {risk_contribution}%")
    print(f"Status: {status}")

if __name__ == "__main__":
    main()
